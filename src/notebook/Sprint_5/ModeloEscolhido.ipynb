{"cells":[{"cell_type":"markdown","metadata":{"id":"HQr15DINUXrN"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"LzMkVheCcsMU"},"source":["A configuração de setup é o processo de preparar e organizar o ambiente para uso. Envolvendo a instalação de bibliotecas e configuração de outros ajustes necessários. O objetivo é criar um ambiente funcional para executar tarefas específicas."]},{"cell_type":"markdown","metadata":{"id":"NVO_GU7gPM4u"},"source":["## 1.1.Conexão com drive"]},{"cell_type":"markdown","metadata":{"id":"q63jKOfNEA9z"},"source":["Para realizar a análise, padronização e manipulação dos dados é necessário selecionar a base de dados desejada. Neste documento a importação da mesma será feita através do Google Drive e o arquivo está em formato excel (csv)."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"J6VYuwXbT0KP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461350657,"user_tz":180,"elapsed":19420,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"ad94dab8-2ab6-4098-b4ae-af818112ab95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"t25UXuE1TXa2"},"source":["## 1.2. Instalando as bibliotecas"]},{"cell_type":"markdown","metadata":{"id":"y-BD7KJcgkAI"},"source":["**Antes de Importar a base é necessário instalar algumas bibliotecas como:**\n","1. Pandas: é uma biblioteca de análise\n","de dados em Python que oferece estruturas de dados e ferramentas para manipulação e análise de dados. Com o Pandas, é possível ler, escrever e manipular dados em diversos formatos, como CSV, Excel, SQL, etc. Ele oferece uma grande quantidade de funcionalidades para trabalhar com dados em Python, incluindo a capacidade de filtrar, agregar, reorganizar e transformar dados de várias maneiras.\n","\n","2. TextBlob: é uma biblioteca de processamento de linguagem natural em Python. Ela oferece uma interface simples para tarefas comuns de NLP, como análise de sentimentos, correção ortográfica, extração de frases-chave e classificação de texto. O TextBlob é construído sobre a biblioteca NLTK (Natural Language Toolkit) e oferece uma sintaxe fácil de usar para muitas tarefas de processamento de linguagem natural. É uma biblioteca muito útil para análise de texto em Python.\n","\n","3. Emoji: é uma biblioteca Python que permite a conversão de emojis de texto para representações de texto Unicode e vice-versa. Além disso, fornece funções úteis para trabalhar com emojis, como contar o número de emojis em uma string, remover todos os emojis de uma string e substituir cada emoji em uma string por um texto de substituição especificado.\n","\n","4. Re: A biblioteca \"re\" é uma ferramenta para trabalhar com expressões regulares, permitindo a extração e manipulação de informações de forma eficiente.\n","\n","5. Unidecode: Unicode é um padrão de codificação de caracteres de forma universal para a representação de caracteres de todas as línguas escritas, bem como símbolos matemáticos, musicais, entre outros.\n","\n","6. Enelvo.normaliser: A enelvo.normaliser é uma biblioteca específica para normalização de texto em português. Ela oferece recursos para corrigir erros de digitação, converter abreviações e substituir palavras informais por suas formas mais corretas. A função principal dessa biblioteca é auxiliar na normalização de texto, tornando-o mais padronizado e compreensível.\n","\n","7. Numpy (np): A biblioteca numpy fornece estruturas de dados eficientes para manipulação e cálculos numéricos, além de funções matemáticas de alto desempenho.\n","\n","8. Sklearn.feature_extraction.text.CountVectorizer: É uma classe do scikit-learn que converte uma coleção de documentos de texto em uma matriz de contagem de tokens. Cada documento é representado por um vetor onde cada elemento é o número de ocorrências de um token específico no documento.\n","\n","9. Keras.preprocessing.text.Tokenizer: É uma classe do Keras que é usada para pré-processamento de texto. Ela converte texto em sequências de números inteiros (índices de palavras) ou matrizes esparsas. Também oferece recursos como tokenização, vetorização e indexação de palavras.\n","\n","10. Nltk.tokenize.word_tokenize: É uma função do NLTK (Natural Language Toolkit) que divide um texto em uma lista de palavras ou tokens. É uma forma de tokenização que leva em consideração o contexto linguístico para separar as palavras."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xi1aSa09Hf_7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461364416,"user_tz":180,"elapsed":13766,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"7cd92a11-29f4-464b-91d6-3a02aeb290da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.6\n"]}],"source":["pip install unidecode"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yeNiAjFUHg1J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461377800,"user_tz":180,"elapsed":13393,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"de152fd5-05c0-4125-8ebd-fb4fd642f7fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji\n","  Downloading emoji-2.5.1.tar.gz (356 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.3/356.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.5.1-py2.py3-none-any.whl size=351210 sha256=34e2bf2d45e118192d2fe6b70f3559c43babc238314d9defa6adede951eb5333\n","  Stored in directory: /root/.cache/pip/wheels/51/92/44/e2ef13f803aa08711819357e6de0c5fe67b874671141413565\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-2.5.1\n"]}],"source":["pip install -U emoji"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rxenMineHlHI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461386108,"user_tz":180,"elapsed":8315,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"5adb2faf-f742-42e4-cab2-902372940c2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.5.1)\n"]}],"source":["!pip install emoji --upgrade"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"J-Si7wySZAV8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461406172,"user_tz":180,"elapsed":20085,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"96ce6048-6409-48f5-a102-0652f4602c69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jSvQ4UyqNlWY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461448683,"user_tz":180,"elapsed":42533,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"03761773-041d-4d9a-81d8-d0453bd02e8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting enelvo\n","  Downloading enelvo-0.15-py3-none-any.whl (27.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: editdistance>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from enelvo) (0.6.2)\n","Collecting emoji==2.2.0 (from enelvo)\n","  Downloading emoji-2.2.0.tar.gz (240 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gensim>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from enelvo) (4.3.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from enelvo) (1.22.4)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from enelvo) (0.8.10)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.1.2->enelvo) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.1.2->enelvo) (6.3.0)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234911 sha256=bfc5fd16d4cc7d123b1c13915ca14053ae2d74adecdadb7cb65b42ed1cd2a387\n","  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n","Successfully built emoji\n","Installing collected packages: emoji, enelvo\n","  Attempting uninstall: emoji\n","    Found existing installation: emoji 2.5.1\n","    Uninstalling emoji-2.5.1:\n","      Successfully uninstalled emoji-2.5.1\n","Successfully installed emoji-2.2.0 enelvo-0.15\n"]}],"source":["pip install enelvo"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3HdQYdYSDkuR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461476909,"user_tz":180,"elapsed":28251,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"2a83d0e7-c0ce-4dbe-af24-1c1886f73be0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import pandas as pd\n","from textblob import TextBlob\n","import emoji\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.snowball import SnowballStemmer\n","nltk.download('wordnet')\n","import re\n","import unidecode\n","from enelvo.normaliser import Normaliser\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from unidecode import unidecode\n","\n","\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2LTgYPWZH-pt","executionInfo":{"status":"ok","timestamp":1687461477322,"user_tz":180,"elapsed":433,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","\n","from sklearn.metrics import confusion_matrix\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","from gensim.models import Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"msXEMaKp5Alc"},"source":["## 1.3.Lendo CSV"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"uwpJdw4n48xG","executionInfo":{"status":"ok","timestamp":1687461478427,"user_tz":180,"elapsed":1109,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["dfW2Vmodelo2 = pd.read_csv('/content/drive/MyDrive/PROJETO BTG/sentence_vectors2.csv')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"5fegaJTqJvSD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687461478429,"user_tz":180,"elapsed":34,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"e5cc3172-882a-4d8f-f3a1-cd061cf88ddd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["11415"]},"metadata":{},"execution_count":10}],"source":["len(dfW2Vmodelo2)"]},{"cell_type":"markdown","metadata":{"id":"iIT1Fx7GEIZ1"},"source":["A célula de código abaixo é responsável por realizar a leitura e apresentação dos dados carregados da base na etapa anterior.\n","\n","**dados** - Apresenta a planilha dos comentários das campanhas de marketing do instagram do BTG Pactual"]},{"cell_type":"markdown","metadata":{"id":"IfcZmhY1lSBc"},"source":["# Atividade 2 - Implementação do Embedding Layer com Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"_vGkDkUnxo6B"},"source":["O modelo de aprendizado profundo utilizado, que inclui as camadas de incorporação (Embedding), convolução unidimensional (Conv1D), pooling máximo global (GlobalMaxPooling1D) e camadas densas (Dense), traz benefícios significativos para o Processamento de Linguagem Natural (PLN).\n","\n","Essas camadas são projetadas para lidar especificamente com dados textuais e têm vantagens específicas:\n","\n","A camada de incorporação (Embedding) permite a representação eficiente de palavras em vetores densos de tamanho fixo, capturando relações semânticas e contextuais entre as palavras. Isso é fundamental para o PLN, pois possibilita que o modelo compreenda melhor as similaridades e diferenças entre as palavras em um texto, melhorando a capacidade de generalização e entendimento do contexto.\n","\n","A camada convolucional unidimensional (Conv1D) é capaz de identificar padrões locais em sequências de palavras, como combinações específicas de palavras ou frases. Isso permite que o modelo extraia características relevantes dos dados textuais, capturando informações importantes em diferentes níveis de granularidade. A convolução unidimensional é especialmente útil para identificar características importantes em trechos curtos de texto.\n","\n","A camada de pooling máximo global (GlobalMaxPooling1D) reduz a dimensionalidade dos recursos extraídos pela camada convolucional, mantendo as características mais importantes. Essa operação de pooling preserva as informações mais salientes, tornando o modelo mais robusto a variações na posição das características dentro das sequências. Além disso, o pooling máximo global permite que o modelo trabalhe com sequências de comprimentos variáveis, já que retorna um vetor de características fixo independentemente do tamanho da sequência original.\n","\n","As camadas densas (Dense) são responsáveis por aprender padrões mais complexos nos recursos extraídos pelas camadas anteriores. Elas permitem que o modelo capture relações não lineares entre as características, aumentando a capacidade de representação e aprendizado. As camadas densas são cruciais para a tomada de decisões finais do modelo, mapeando os recursos em probabilidades de pertencer a cada classe e permitindo a classificação adequada dos dados textuais.\n","\n","Portanto, o uso dessas camadas no modelo de aprendizado profundo para PLN traz ganhos significativos, permitindo que o modelo compreenda melhor o contexto, identifique padrões relevantes nas sequências de palavras e capture informações importantes para a classificação precisa dos dados textuais. Essa arquitetura é especialmente útil para tarefas como classificação de sentimentos, análise de sentimento, categorização de texto e muitas outras aplicações de PLN.\n","\n","**Referências**\n","\n","(1) What Are Embedding Layers in Neural Networks? - Baeldung.\n","\n","https://www.baeldung.com/cs/neural-nets-embedding-layers.\n","\n","(2) Understanding Embedding Layer in Keras | by sawan saxena - Medium.\n","\n","https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce.\n","\n","(3) Embedding layer - Keras.\n","\n","https://keras.io/api/layers/core_layers/embedding/."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Eil4-TOmBoO-","executionInfo":{"status":"ok","timestamp":1687461478430,"user_tz":180,"elapsed":33,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.preprocessing.text import Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"yUNwEmXgw7F8"},"source":["## 2.1 - Definição da Função"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"0d4Ef6GaxUC5","executionInfo":{"status":"ok","timestamp":1687461478431,"user_tz":180,"elapsed":33,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["def classification_embedding_layer(X, y):\n","  # Separar em conjuntos de treinamento e teste\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","  # Tokenizar os dados\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(X_train)\n","  X_train = tokenizer.texts_to_sequences(X_train)\n","  X_test = tokenizer.texts_to_sequences(X_test)\n","\n","  # Obter o tamanho do vocabulário\n","  vocab_size = len(tokenizer.word_index) + 1\n","\n","  # Padding para ter sequências com o mesmo comprimento\n","  max_length = 50\n","  X_train = pad_sequences(X_train, maxlen=max_length)\n","  X_test = pad_sequences(X_test, maxlen=max_length)\n","\n","  # Criar o modelo\n","  model = Sequential()\n","  model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=max_length))\n","  model.add(LSTM(128))\n","  model.add(Dense(3, activation='softmax'))\n","\n","  # Compilar o modelo\n","  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  # Treinar o modelo\n","  model.fit(X_train, y_train, epochs=10, batch_size=32)\n","\n","  # Fazer previsões no conjunto de teste\n","  y_pred = model.predict(X_test)\n","  y_pred = np.argmax(y_pred, axis=1)\n","\n","  # Printar o relatório de classificação (incluindo recall)\n","  labels = ['Negative', 'Positive', 'Neutral']\n","\n","  # Avaliar a precisão do modelo\n","  _, accuracy = model.evaluate(X_test, y_test)\n","  print('Acurácia: %.2f%%' % (accuracy * 100))\n","\n","  report = classification_report(y_test, y_pred, target_names=labels)\n","  print(report)\n","\n","  return model, y_test, y_pred"]},{"cell_type":"markdown","metadata":{"id":"X6UCdQuiCTNm"},"source":["## Embedding Layer - Duas categorias e com balanceamento"]},{"cell_type":"markdown","metadata":{"id":"5bzwOU_GxL18"},"source":["### 2.2 - Teste de Função"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"K-QWODi0xaKa","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1687461478432,"user_tz":180,"elapsed":33,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"45c4da0b-8f78-4fec-853b-f9c8a2ee0450"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# Balanceamento dos dados\\ndf_positive = dfW2Vmodelo[dfW2Vmodelo['sentimento'] == 0]\\ndf_negative = dfW2Vmodelo[dfW2Vmodelo['sentimento'] == 1]\\n\\ndf_positive_resampled = resample(df_positive, replace=True, n_samples=len(df_negative), random_state=42)\\n\\ndfrnn_balanced = pd.concat([df_positive_resampled, df_negative])\\n\\n# Inverter a codificação das classes\\ndfrnn_balanced['sentimento'] = dfrnn_balanced['sentimento'].map({1: 0, 0: 1})\\n\\n# Separando os dados em X e y (balanceados)\\nX = np.array(dfrnn_balanced['Frase'])\\nY = np.array(dfrnn_balanced['sentimento'])\\n\\n# Definição dos hiperparâmetros\\nnum_epochs = 20\\nbatch_size = 64\\nlearning_rate = 0.001\\nnum_hidden_units = 128\\nembedding_dim = 50\\n\\nloss, accuracy, y_pred, y_test, X_test = classification_embedding_layer(X, Y, num_epochs, batch_size, learning_rate, num_hidden_units, embedding_dim)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["\"\"\"\n","# Balanceamento dos dados\n","df_positive = dfW2Vmodelo[dfW2Vmodelo['sentimento'] == 0]\n","df_negative = dfW2Vmodelo[dfW2Vmodelo['sentimento'] == 1]\n","\n","df_positive_resampled = resample(df_positive, replace=True, n_samples=len(df_negative), random_state=42)\n","\n","dfrnn_balanced = pd.concat([df_positive_resampled, df_negative])\n","\n","# Inverter a codificação das classes\n","dfrnn_balanced['sentimento'] = dfrnn_balanced['sentimento'].map({1: 0, 0: 1})\n","\n","# Separando os dados em X e y (balanceados)\n","X = np.array(dfrnn_balanced['Frase'])\n","Y = np.array(dfrnn_balanced['sentimento'])\n","\n","# Definição dos hiperparâmetros\n","num_epochs = 20\n","batch_size = 64\n","learning_rate = 0.001\n","num_hidden_units = 128\n","embedding_dim = 50\n","\n","loss, accuracy, y_pred, y_test, X_test = classification_embedding_layer(X, Y, num_epochs, batch_size, learning_rate, num_hidden_units, embedding_dim)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"vGw0sCRL9LTe"},"source":["### 2.3 - Visualização da Matriz de Confusão do Embedding Layer"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6PCIVYr49PgZ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1687461478433,"user_tz":180,"elapsed":32,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"980ddd9c-2caa-43a8-acf0-39d18d046ee3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Calcular a matriz de confusão\\nconfusion = confusion_matrix(y_pred,y_test)\\n\\nprint(\"Matriz de Confusão:\")\\nprint(confusion)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["\"\"\"\n","# Calcular a matriz de confusão\n","confusion = confusion_matrix(y_pred,y_test)\n","\n","print(\"Matriz de Confusão:\")\n","print(confusion)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"v-GeH26Bz6AG"},"source":["A matriz de confusão que você obteve mostra os seguintes resultados:\n","\n","- Verdadeiros Positivos (TP): 127\n","- Falsos Positivos (FP): 38\n","- Falsos Negativos (FN): 25\n","- Verdadeiros Negativos (TN): 141"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"06FDHrbV1cro","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1687461478433,"user_tz":180,"elapsed":30,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"75dc985c-83b2-4306-94e9-fba138d133ee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# Cálculo da matriz de confusão\\ncm = confusion_matrix(y_test, y_pred)\\n\\n# Definição dos rótulos das classes\\nclass_labels = ['POSITIVO', 'NEGATIVO']\\n\\n# Criação da matriz de confusão personalizada\\ncm_custom = np.array([[cm[1, 1], cm[1, 0]], [cm[0, 1], cm[0, 0]]])\\n\\n# Plot da matriz de confusão\\nplt.figure(figsize=(8, 6))\\nplt.imshow(cm_custom, interpolation='nearest', cmap=plt.cm.Blues)\\nplt.title('Matriz de Confusão do Embedding Layer com Word2Vec')\\nplt.colorbar()\\ntick_marks = np.arange(len(class_labels))\\nplt.xticks(tick_marks, class_labels)\\nplt.yticks(tick_marks, class_labels)\\n\\n# Adição dos valores numéricos na matriz\\nthresh = cm_custom.max() / 2.0\\nfor i, j in np.ndindex(cm_custom.shape):\\n    plt.text(j, i, format(cm_custom[i, j], 'd'), ha='center', va='center',\\n             color='white' if cm_custom[i, j] > thresh else 'black')\\n\\nplt.xlabel('Previsão')\\nplt.ylabel('Valor Real')\\nplt.tight_layout()\\nplt.show()\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["\"\"\"\n","# Cálculo da matriz de confusão\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Definição dos rótulos das classes\n","class_labels = ['POSITIVO', 'NEGATIVO']\n","\n","# Criação da matriz de confusão personalizada\n","cm_custom = np.array([[cm[1, 1], cm[1, 0]], [cm[0, 1], cm[0, 0]]])\n","\n","# Plot da matriz de confusão\n","plt.figure(figsize=(8, 6))\n","plt.imshow(cm_custom, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title('Matriz de Confusão do Embedding Layer com Word2Vec')\n","plt.colorbar()\n","tick_marks = np.arange(len(class_labels))\n","plt.xticks(tick_marks, class_labels)\n","plt.yticks(tick_marks, class_labels)\n","\n","# Adição dos valores numéricos na matriz\n","thresh = cm_custom.max() / 2.0\n","for i, j in np.ndindex(cm_custom.shape):\n","    plt.text(j, i, format(cm_custom[i, j], 'd'), ha='center', va='center',\n","             color='white' if cm_custom[i, j] > thresh else 'black')\n","\n","plt.xlabel('Previsão')\n","plt.ylabel('Valor Real')\n","plt.tight_layout()\n","plt.show()\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"m_ubgUmK_KSz"},"source":["### 2.4 - Salvando CSV do resultado Embedding Layer com Word2Vec"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"xixVZaZZ-KvX","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1687461478434,"user_tz":180,"elapsed":29,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"7448fe7c-21cc-4fda-84d1-f080f541e933"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport pandas as pd\\n\\n# Converter X_test em lista e y_pred em array unidimensional\\nX_test_list = X_test.tolist()\\ny_pred_flat = y_pred.flatten()\\n\\n# Criar um DataFrame com as previsões\\ndf_resultEL = pd.DataFrame({\\'Texto\\': X_test_list, \\'Sentimento_Predito\\': y_pred_flat})\\n\\n# Salvar o DataFrame em um arquivo CSV\\ndf_resultEL.to_csv(\\'/content/drive/MyDrive/PROJETO BTG/resultados.csv\\', index=False)\\nprint(\"Salvo com sucesso\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["\"\"\"\n","import pandas as pd\n","\n","# Converter X_test em lista e y_pred em array unidimensional\n","X_test_list = X_test.tolist()\n","y_pred_flat = y_pred.flatten()\n","\n","# Criar um DataFrame com as previsões\n","df_resultEL = pd.DataFrame({'Texto': X_test_list, 'Sentimento_Predito': y_pred_flat})\n","\n","# Salvar o DataFrame em um arquivo CSV\n","df_resultEL.to_csv('/content/drive/MyDrive/PROJETO BTG/resultados.csv', index=False)\n","print(\"Salvo com sucesso\")\n","\"\"\""]},{"cell_type":"code","execution_count":17,"metadata":{"id":"YCJ6bPEYW1qx","executionInfo":{"status":"ok","timestamp":1687461478785,"user_tz":180,"elapsed":379,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["# df_resultEL"]},{"cell_type":"markdown","metadata":{"id":"73fgorD8CcGr"},"source":["## Embedding Layer - Três categorias e sem balanceamento"]},{"cell_type":"markdown","metadata":{"id":"XGLEYCwXCcG4"},"source":["### 2.2 - Teste de Função"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"08eFwjrQCcG4","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"error","timestamp":1687461638979,"user_tz":180,"elapsed":160197,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"32f8bd0c-26bd-4cb2-9cb1-6d78bd86394e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","250/250 [==============================] - 64s 232ms/step - loss: 0.8651 - accuracy: 0.5984\n","Epoch 2/10\n","250/250 [==============================] - 33s 132ms/step - loss: 0.4860 - accuracy: 0.8143\n","Epoch 3/10\n","250/250 [==============================] - 34s 135ms/step - loss: 0.2531 - accuracy: 0.9113\n","Epoch 4/10\n","205/250 [=======================>......] - ETA: 5s - loss: 0.1489 - accuracy: 0.9492"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-6ac0eef6fd30>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfrnn_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentimento2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-07ec68330cbe>\u001b[0m in \u001b[0;36mclassification_embedding_layer\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# Treinar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# Fazer previsões no conjunto de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Carregar os dados\n","df_negative = dfW2Vmodelo2[dfW2Vmodelo2['sentimento2'] == 0]\n","df_positive = dfW2Vmodelo2[dfW2Vmodelo2['sentimento2'] == 1]\n","df_neutral = dfW2Vmodelo2[dfW2Vmodelo2['sentimento2'] == 2]\n","\n","dfrnn_balanced = pd.concat([df_positive, df_negative, df_neutral])\n","\n","# Separar os dados em X e y (balanceados)\n","X = dfrnn_balanced['Frase'].values\n","y = dfrnn_balanced['sentimento2'].values\n","\n","model, y_test, y_pred = classification_embedding_layer(X, y)"]},{"cell_type":"markdown","metadata":{"id":"n5d3hFyoCcG5"},"source":["### 2.3 - Visualização da Matriz de Confusão do Embedding Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nA4k-STSCcG5","executionInfo":{"status":"aborted","timestamp":1687461638982,"user_tz":180,"elapsed":168,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["# Criar matriz de confusão\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Printar a matriz de confusão\n","labels = ['NEGATIVO', 'POSITIVO', 'NEUTRO']\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n","\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"f0CGVtKuvt5c"},"source":["### 2.4 - Hiperparâmetros com Random Search do Embedding Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBFp_dOfv2rg","executionInfo":{"status":"aborted","timestamp":1687461638982,"user_tz":180,"elapsed":167,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import recall_score\n","\n","def create_model(embedding_dim, vocab_size, max_length):\n","    # Criação do modelo\n","    model = Sequential([\n","        Embedding(vocab_size, embedding_dim, input_length=max_length),\n","        Conv1D(32, 3, activation='relu'),\n","        GlobalMaxPooling1D(),\n","        Dense(units=128, activation='relu'),\n","        Dense(units=3, activation='softmax')  # Altere o número de unidades para o número de classes\n","    ])\n","\n","    # Compilação do modelo\n","    model.compile(optimizer=Adam(learning_rate=0.001),\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","# Dados de entrada\n","X = dfrnn_balanced['Frase'].values\n","Y = dfrnn_balanced['sentimento2'].values\n","\n","# Tokenização\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","X_seq = tokenizer.texts_to_sequences(X)\n","max_length = max(len(seq) for seq in X_seq)\n","X_padded = pad_sequences(X_seq, maxlen=max_length)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Melhores hiperparâmetros encontrados\n","best_embedding_dim = 100\n","best_batch_size = 32\n","\n","# Criação do modelo com os melhores hiperparâmetros\n","model = create_model(best_embedding_dim, vocab_size, max_length)\n","\n","# Treinamento do modelo\n","model.fit(X_padded, Y, batch_size=best_batch_size, epochs=10, validation_split=0.2)\n","\n","# Avaliação do modelo\n","y_pred = model.predict(X_padded)\n","y_pred = np.argmax(y_pred, axis=1)  # Converter as previsões para as classes numéricas\n","recall = recall_score(Y, y_pred, average='weighted')\n","\n","print('Recall:', recall)\n"]},{"cell_type":"markdown","metadata":{"id":"o6rOssjn_vzd"},"source":["### 2.4.1 - Embedding Layer com os Melhores Hiperparâmetros"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"8xMIvIEH_8CN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687462054318,"user_tz":180,"elapsed":329940,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}},"outputId":"0736199f-e48f-47fb-c70a-7c6c0337f9a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","286/286 [==============================] - 42s 140ms/step - loss: -2.9324 - accuracy: 0.5059 - recall_2: 0.9829 - val_loss: -240.6554 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9834\n","Epoch 2/10\n","286/286 [==============================] - 27s 93ms/step - loss: -552.6476 - accuracy: 0.5470 - recall_2: 0.9781 - val_loss: -14923.7266 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9790\n","Epoch 3/10\n","286/286 [==============================] - 31s 109ms/step - loss: -6838.6538 - accuracy: 0.5544 - recall_2: 0.9811 - val_loss: -109070.1328 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9799\n","Epoch 4/10\n","286/286 [==============================] - 28s 98ms/step - loss: -32746.8281 - accuracy: 0.5584 - recall_2: 0.9855 - val_loss: -390816.2812 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9869\n","Epoch 5/10\n","286/286 [==============================] - 28s 98ms/step - loss: -96342.1562 - accuracy: 0.5551 - recall_2: 0.9871 - val_loss: -979250.5625 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9890\n","Epoch 6/10\n","286/286 [==============================] - 29s 102ms/step - loss: -218490.2656 - accuracy: 0.5576 - recall_2: 0.9879 - val_loss: -2011750.7500 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9882\n","Epoch 7/10\n","286/286 [==============================] - 30s 103ms/step - loss: -421667.8125 - accuracy: 0.5619 - recall_2: 0.9893 - val_loss: -3577480.7500 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9886\n","Epoch 8/10\n","286/286 [==============================] - 28s 99ms/step - loss: -727815.8125 - accuracy: 0.5592 - recall_2: 0.9896 - val_loss: -5845759.5000 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9882\n","Epoch 9/10\n","286/286 [==============================] - 28s 96ms/step - loss: -1166490.5000 - accuracy: 0.5620 - recall_2: 0.9893 - val_loss: -9037310.0000 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9882\n","Epoch 10/10\n","286/286 [==============================] - 28s 98ms/step - loss: -1764982.2500 - accuracy: 0.5629 - recall_2: 0.9902 - val_loss: -13196296.0000 - val_accuracy: 0.0000e+00 - val_recall_2: 0.9895\n","357/357 [==============================] - 4s 12ms/step - loss: -4365280.5000 - accuracy: 0.4466 - recall_2: 0.9917\n","Recall: 0.9916769862174988\n"]}],"source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import recall_score\n","\n","def create_model(vocab_size, max_length):\n","    # Criação do modelo\n","    model = Sequential([\n","        Embedding(vocab_size, 100, input_length=max_length),  # Aplicação do melhor hiperparâmetro\n","        Conv1D(32, 3, activation='relu'),\n","        GlobalMaxPooling1D(),\n","        Dense(units=128, activation='relu'),\n","        Dense(units=1, activation='sigmoid')\n","    ])\n","\n","    # Compilação do modelo\n","    model.compile(optimizer=Adam(learning_rate=0.001),\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy', tf.keras.metrics.Recall()])\n","\n","    return model\n","\n","# Dados de entrada\n","X = dfrnn_balanced['Frase'].values\n","Y = dfrnn_balanced['sentimento2'].values\n","\n","# Tokenização\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","X_seq = tokenizer.texts_to_sequences(X)\n","max_length = max(len(seq) for seq in X_seq)\n","X_padded = pad_sequences(X_seq, maxlen=max_length)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Criação do modelo com o melhor hiperparâmetro\n","model = create_model(vocab_size, max_length)\n","\n","# Treinamento do modelo\n","model.fit(X_padded, Y, batch_size=32, epochs=10, validation_split=0.2)\n","\n","# Avaliação do modelo\n","loss, accuracy, recall = model.evaluate(X_padded, Y)\n","\n","ELRecall = recall;\n","\n","print('Recall:', ELRecall)\n"]},{"cell_type":"code","source":["# Dados de entrada\n","X = dfrnn_balanced['Frase'].values\n","Y = dfrnn_balanced['sentimento2'].values\n","\n","# Tokenização\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X)\n","X_seq = tokenizer.texts_to_sequences(X)\n","max_length = 50\n","X_padded = pad_sequences(X_seq, maxlen=max_length)\n","vocab_size = len(tokenizer.word_index) + 1"],"metadata":{"id":"F9k-h4Q_YaJg","executionInfo":{"status":"ok","timestamp":1687462056310,"user_tz":180,"elapsed":930,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["max_length"],"metadata":{"id":"yEzmB1NvZImp","executionInfo":{"status":"aborted","timestamp":1687461638985,"user_tz":180,"elapsed":167,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_padded"],"metadata":{"id":"mn2L0tY9Y-dk","executionInfo":{"status":"aborted","timestamp":1687461638986,"user_tz":180,"elapsed":168,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"DQjnbgUiYegp","executionInfo":{"status":"aborted","timestamp":1687461638986,"user_tz":180,"elapsed":167,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Amh4Of9JCcG5"},"source":["### 2.5 - Salvando CSV do resultado Embedding Layer com Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2-4d49HCcG5","executionInfo":{"status":"aborted","timestamp":1687461638991,"user_tz":180,"elapsed":168,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["\"\"\"\n","# Criar DataFrame com as previsões\n","df_predictions = pd.DataFrame({'Sentimento Real': y_test, 'Sentimento Previsto': y_pred})\n","\n","# Salvar o DataFrame em um arquivo CSV\n","df_predictions.to_csv('predictionsEmbeddingLayer.csv', index=False)\n","print(\"Salvo com sucesso\")\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"j5CqaP5JLg7D"},"source":["### 2.6 - Exportar o Modelo Embedding Layer com pickle\n"]},{"cell_type":"code","source":["#biblioteca para exportação dos modelos\n","import pickle"],"metadata":{"id":"w_ITxtTuSlmZ","executionInfo":{"status":"aborted","timestamp":1687461638994,"user_tz":180,"elapsed":171,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fOcNJ6jWLuFO","executionInfo":{"status":"aborted","timestamp":1687461638995,"user_tz":180,"elapsed":171,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["# Salvar o modelo RNN em um arquivo PKL\n","with open('/content/drive/MyDrive/PROJETO BTG/modelo_embedding_layer.pkl', 'wb') as arquivo:\n","    pickle.dump(model, arquivo)"]},{"cell_type":"markdown","metadata":{"id":"ejkDLPNtCeXa"},"source":["## Embedding Layer - Três categorias e com balanceamento"]},{"cell_type":"markdown","metadata":{"id":"eaFKgol-CeXm"},"source":["### 2.2 - Teste de Função"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yq5225aSCeXn","executionInfo":{"status":"aborted","timestamp":1687461638995,"user_tz":180,"elapsed":173,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["# Balanceamento dos dados\n","df_negative = dfW2Vmodelo2[dfW2Vmodelo2['sentimento2'] == 0]\n","df_positive = dfW2Vmodelo2[dfW2Vmodelo2['sentimento2'] == 1]\n","df_neutral = dfW2Vmodelo2[dfW2Vmodelo2['sentimento2'] == 2]\n","\n","df_positive_resampled = resample(df_positive, replace=True, n_samples=len(df_negative), random_state=42)\n","df_neutral_resampled = resample(df_neutral, replace=True, n_samples=len(df_negative), random_state=42)\n","\n","dfrnn_balanced = pd.concat([df_positive_resampled, df_negative, df_neutral_resampled])\n","\n","# Separar os dados em X e y (balanceados)\n","X = dfrnn_balanced['Frase'].values\n","y = dfrnn_balanced['sentimento2'].values\n","\n","model, y_test, y_pred = classification_embedding_layer(X, y)"]},{"cell_type":"markdown","metadata":{"id":"YjRiE187CeXn"},"source":["### 2.3 - Visualização da Matriz de Confusão do Embedding Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SjmjeWVCeXn","executionInfo":{"status":"aborted","timestamp":1687461638999,"user_tz":180,"elapsed":171,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["import seaborn as sns\n","\n","# Calcular a matriz de confusão\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Definição dos rótulos das classes\n","class_labels = ['POSITIVO', 'NEGATIVO', 'NEUTRO']\n","\n","# Plot da matriz de confusão\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n","plt.xlabel('Predito')\n","plt.ylabel('Verdadeiro')\n","plt.title('Matriz de Confusão')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eMimutz-CeXn"},"source":["### 2.4 - Salvando CSV do resultado Embedding Layer com Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3xRLhcVCeXo","executionInfo":{"status":"aborted","timestamp":1687461639000,"user_tz":180,"elapsed":308810,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"outputs":[],"source":["# df_resultEL"]},{"cell_type":"markdown","source":["# Teste do Modelo"],"metadata":{"id":"9vP5ZRpUTCI9"}},{"cell_type":"markdown","source":["## Pré-Processamento"],"metadata":{"id":"0JGPcPQ7U3_S"}},{"cell_type":"code","source":["import pickle\n","import pandas as pd\n","\n","# Carregar o arquivo pickle\n","with open('preprocessamento.pkl', 'rb') as f:\n","    preprocessamento = pickle.load(f)\n","\n","# Carregar o arquivo CSV\n","frases = pd.read_csv('frases.csv')"],"metadata":{"id":"14p3r4-QTGlX","executionInfo":{"status":"aborted","timestamp":1687461639001,"user_tz":180,"elapsed":308808,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frase = \"Eu gosto de programar em Python!\""],"metadata":{"id":"WnUYiOzFUp5T","executionInfo":{"status":"aborted","timestamp":1687461639001,"user_tz":180,"elapsed":308802,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resultado = aplicar_preprocessamento(frase, preprocessamento, frases)"],"metadata":{"id":"Gog7urV2UsgV","executionInfo":{"status":"aborted","timestamp":1687461639002,"user_tz":180,"elapsed":308799,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(resultado)"],"metadata":{"id":"orkWHRMZU2v4","executionInfo":{"status":"aborted","timestamp":1687461639003,"user_tz":180,"elapsed":308798,"user":{"displayName":"Lucas Vieira","userId":"06167590725923763905"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}