# -*- coding: utf-8 -*-
"""Pré-Processamento.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-hDmny7DWzs9USeO7ki71dbpjcaW4w3

# BTG Pactual - Análise de sentimentos

**INTEGRANTES:**


1. Dayllan de Souza Alho
2. Eric Tachdjian
3. Gabriela de Morais da Silva
4. Giovanna Furlan Torres
5. Lucas de Britto Vieira
6. Michel Mansur

# Introdução

**1.Problema a ser resolvido:**
O BTG Pactual enfrenta o desafio de otimizar suas estratégias de marketing digital e entender melhor o comportamento e preferências dos consumidores nas redes sociais. Com o aumento do investimento em marketing digital e a crescente utilização das redes sociais, a análise de dados de mídia social é fundamental para obter informações relevantes e tomar decisões de negócios mais eficazes. O objetivo é utilizar PLN para rastrear dados e analisar a receptividade dos usuários às campanhas no Instagram do banco, identificar palavras-chave nos comentários e direcionar novas campanhas baseadas nos interesses dos consumidores.

**2. Solução Proposta**
Sabendo que mais de 50% da população mundial que usa redes sociais por mais de 2 horas por dia e a crescente importância do marketing nas empresas (TAPI, 2023), o BTG Pactual em parceria com o Inteli está desenvolvendo o projeto de "Análise de Sentimento das Campanhas de Marketing em Redes Sociais". Através da tecnologia de Processamento de Linguagem Natural (PLN), será desenvolvido uma ferramenta que ajudará a empresa a compreender a receptividade dos clientes às suas campanhas de marketing e nas tomadas de decisões das áreas de negócios, através da análise de sentimento e identificação de palavras-chave nos comentários dos usuários, permitindo uma resposta rápida a possíveis problemas ou oportunidades.

A análise de sentimentos dos comentários das campanhas de marketing do instagram do BTG Pactual será realizada através do Google Colaboratory, por meio deste notebook. Nas sessões seguintes serão demonstradas passo a passo de como os dados foram utilizados até a conclusão e entrega do modelo esperado, passando pelas fases:


1. Seleção dos dados;
2. Processamento dos dados selecionados;
3. Transformação dos dados (pré-processamento);
4. Teste do modelo; e
5. Interpretação e Avaliação do modelo;

#Setup

A configuração de setup é o processo de preparar e organizar o ambiente para uso. Envolvendo a instalação de bibliotecas e configuração de outros ajustes necessários. O objetivo é criar um ambiente funcional para executar tarefas específicas.

## 1.1.Conexão com drive

Para realizar a análise, padronização e manipulação dos dados é necessário selecionar a base de dados desejada. Neste documento a importação da mesma será feita através do Google Drive e o arquivo está em formato excel (csv).
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""## 1.2. Instalando as bibliotecas

**Antes de Importar a base é necessário instalar algumas bibliotecas como:**
1. Pandas: é uma biblioteca de análise
de dados em Python que oferece estruturas de dados e ferramentas para manipulação e análise de dados. Com o Pandas, é possível ler, escrever e manipular dados em diversos formatos, como CSV, Excel, SQL, etc. Ele oferece uma grande quantidade de funcionalidades para trabalhar com dados em Python, incluindo a capacidade de filtrar, agregar, reorganizar e transformar dados de várias maneiras.

2. TextBlob: é uma biblioteca de processamento de linguagem natural em Python. Ela oferece uma interface simples para tarefas comuns de NLP, como análise de sentimentos, correção ortográfica, extração de frases-chave e classificação de texto. O TextBlob é construído sobre a biblioteca NLTK (Natural Language Toolkit) e oferece uma sintaxe fácil de usar para muitas tarefas de processamento de linguagem natural. É uma biblioteca muito útil para análise de texto em Python.

3. Emoji: é uma biblioteca Python que permite a conversão de emojis de texto para representações de texto Unicode e vice-versa. Além disso, fornece funções úteis para trabalhar com emojis, como contar o número de emojis em uma string, remover todos os emojis de uma string e substituir cada emoji em uma string por um texto de substituição especificado.

4. Re: A biblioteca "re" é uma ferramenta para trabalhar com expressões regulares, permitindo a extração e manipulação de informações de forma eficiente.

5. Unidecode: Unicode é um padrão de codificação de caracteres de forma universal para a representação de caracteres de todas as línguas escritas, bem como símbolos matemáticos, musicais, entre outros.

6. Enelvo.normaliser: A enelvo.normaliser é uma biblioteca específica para normalização de texto em português. Ela oferece recursos para corrigir erros de digitação, converter abreviações e substituir palavras informais por suas formas mais corretas. A função principal dessa biblioteca é auxiliar na normalização de texto, tornando-o mais padronizado e compreensível.

7. Numpy (np): A biblioteca numpy fornece estruturas de dados eficientes para manipulação e cálculos numéricos, além de funções matemáticas de alto desempenho.

8. Sklearn.feature_extraction.text.CountVectorizer: É uma classe do scikit-learn que converte uma coleção de documentos de texto em uma matriz de contagem de tokens. Cada documento é representado por um vetor onde cada elemento é o número de ocorrências de um token específico no documento.

9. Keras.preprocessing.text.Tokenizer: É uma classe do Keras que é usada para pré-processamento de texto. Ela converte texto em sequências de números inteiros (índices de palavras) ou matrizes esparsas. Também oferece recursos como tokenização, vetorização e indexação de palavras.

10. Nltk.tokenize.word_tokenize: É uma função do NLTK (Natural Language Toolkit) que divide um texto em uma lista de palavras ou tokens. É uma forma de tokenização que leva em consideração o contexto linguístico para separar as palavras.
"""

# pip install unidecode

# pip install -U emoji

# !pip install emoji --upgrade

# pip install nltk

# pip install enelvo

import pandas as pd
from textblob import TextBlob
import emoji
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
nltk.download('wordnet')
import re
import unidecode
from enelvo.normaliser import Normaliser
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from unidecode import unidecode


import numpy as np
# from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from nltk.tokenize import word_tokenize
# from sklearn.preprocessing import StandardScaler

import tensorflow as tf
# from sklearn.model_selection import train_test_split
# from sklearn.utils import resample

# from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.sequence import pad_sequences

# from sklearn.naive_bayes import GaussianNB
# from sklearn.metrics import accuracy_score

from gensim.models import Word2Vec

"""## 1.3.Lendo CSV"""

# base = pd.read_csv('/content/drive/MyDrive/PROJETO BTG/BaseDadosBTG2.csv')

"""A célula de código abaixo é responsável por realizar a leitura e apresentação dos dados carregados da base na etapa anterior.

**dados** - Apresenta a planilha dos comentários das campanhas de marketing do instagram do BTG Pactual

## 1.4.Visualização dos tipos de colunas

As colunas apresentadas na base de dados disponibilizada possui tipos diferentes de formatação, sendo divididos em:

1. **float :** Responsável por armazenar números reais com precisão de 6 casas decimais;
2. **object :** Responsável por armazenar qualquer tipo de dado genêrico, utilizado para representar características abstratas;
3. **int64 :** Dado numérico que pode armazenar valores inteiros de até 64 bits.

O código abaixo apresenta os tipos encontrados na planilha BaseBTGM6.
"""

# base.dtypes

"""### 1.4.1 Retirando as "" dos nomes das colunas

Como é possível observar acima as colunas estão nomeadas com aspas, e para um melhor aproveitamento do código é preciso retirar essa pontuação dos nomes.

# #### 1.4.1.1 Teste isolado da remoção das aspas
# """

# #Criando um dataframe para teste
# teste = pd.DataFrame({'"Nome"': ['Gabriela', 'Giovanna'], '"Idade"': [20, 30]})
# print(teste.head())

# #Função de tirar aspas dos nomes das colunas
# def tiraAspas(teste):
#     teste.columns = [col.replace('"', '') for col in teste.columns]
#     return teste

# #Aplicação da função
# teste = tiraAspas(teste)
# print(teste.head())

# """#### 1.4.1.2. Definição da função"""

# def tiraAspas(base):
#     base_rename = base.rename(columns=lambda x: re.sub('[\"\']', '', x))
#     return base_rename

# """#### 1.4.1.3. Aplicação da função tiraAspas"""

# # Aplicando a função na base de dados
# base_rename = tiraAspas(base)
# #Resultado da função
# base_rename

"""### 1.4.2.Explicação das colunas

#### 1.4.2.1.Colunas utilizadas

## 1.5. Base Tratada
# """

# dados = base_rename[['autor', 'texto', 'sentimento', 'tipoInteracao']].copy()

# dados

"""# Explicação das Etapas

## Teste Isolado

Define-se um teste isolado, como o teste do sistema de forma independente, verificando todo o processo específico, sem interferência de outros componentes. A execução é feita de forma separada, com o fornecimento da entrada específica, auxiliando na identificação de problemas, além de permitir "guardar" a lógica utilizada no desenvolvimento do programa em larga escala.

## Teste da Função

Responsável pela verificação do funcionamento da função específica do sistema. Busca Verificar se a função produz os resultados esperados.

Através dela, nesse programa,a criamos a pipeline do sistema. Uma sequência de processos que são executados em uma determinada ordem, buscando modificar no caso, realizar o pré-processamento dos textos fornecidos. Assim, utiliza-se o resultado da etapa anterior como entrada para a próxima. O objetivo é automatizar a execuçã dividindo-as em tarefas menores e mais simples.

# Atividade 1: Remoção de comentários do BTGPactual

## 1.1.Teste isolado da remoção de comentários do BTGPactual
"""

# # Criando um DataFrame para o teste
# teste = pd.DataFrame({
#     'autor': ['@btgPactual', '@inteli', '@btgPactual', '@btgPactual', '@inteli', '@SisitemasDeInformação'],
#     'texto': ['Obrigada pelo feedback', 'Poderia Melhorar as transações do pix', 'Obrigada pelo feedback', 'Obrigada pelo feedback', 'Gosto muito no BTG', 'Sistemas é o melhor curso.']
# })

# print(teste)

# Função de retirar comentários do BTGPactual
# def filtrar_dados_por_autor(teste, autor):
    # Filtra os dados por autor, removendo as linhas em que a coluna 'autor' contém a string passada como parâmetro.
    # Substitui os valores nulos (NaN) da coluna 'autor' por 'desconhecido'.
    # Retorna um novo DataFrame com as linhas filtradas.
#     # Substitui os valores nulos (NaN) da coluna 'autor' por 'desconhecido'.
#     teste['autor'] = teste['autor'].fillna('desconhecido')

#     # Cria um novo DataFrame chamado 'dados_filtrados', que contém todas as linhas do DataFrame original (dados), exceto aquelas em que a coluna 'autor' contém a string passada como parâmetro.
#     # O operador ~ é usado para selecionar todas as linhas em que a condição 'dados['autor'].str.contains(autor)' é falsa.
#     dados_filtrados = teste[~teste['autor'].str.contains(autor)]

#     return dados_filtrados

# dados_filtrados = filtrar_dados_por_autor(teste, '@btgPactual')
# print(dados_filtrados)

"""## 1.2. Definição de Função

Função de remoção de autor
"""

# def filtrar_dados_por_autor(dados, autor_anonimo):
#     # Converter a coluna 'autor_anonimo' para o tipo string
#     dados['autor'] = dados['autor'].astype(str)

#     # Filtrar os dados por autor, removendo as linhas em que a coluna 'autor_anonimo' contém a string passada como parâmetro
#     # Substituir os valores nulos (NaN) da coluna 'autor_anonimo' por 'desconhecido'
#     dados_anonimos = dados[~dados['autor'].str.contains(autor_anonimo, na=False)]

#     return dados_anonimos

"""## 1.3. Testes da Função de Remoção de autor"""

# dados_filtrados = filtrar_dados_por_autor(dados, 'autor_anonimo')
# dados_filtrados

# """## 1.4 - Salvando CSV da Remoção de Autor"""

# # Chamar a função para filtrar os dados por autor anônimo/content/drive/MyDrive/PROJETO BTG/dados_filtrados.csv
# autor_anonimo = 'desconhecido'  # Substitua pela string que representa o autor anônimo
# dados_filtrados = filtrar_dados_por_autor(dados, autor_anonimo)

# # Verificar o resultado
# print(dados_filtrados.head())

# # Salvar os dados filtrados em um novo arquivo CSV
# dados_filtrados.to_csv('/content/drive/MyDrive/PROJETO BTG/remocaoautor.csv', index=False)
# print('Dados filtrados salvos com sucesso.')

# dados_filtrados

import pandas as pd
import re
from unidecode import unidecode
import emoji

# Carregar o arquivo CSV
dados_filtrados = pd.read_csv('remocaoautor.csv')

# Função de remoção de caracteres especiais
def removeCaracteres(text):
    if isinstance(text, str):
        return re.sub(r'[^\w\s]', '', text)
    else:
        return text

# Função de remoção de acentos
def removeAcentos(x):
    return unidecode(x) if isinstance(x, str) else x

# Função de remoção de emojis
def removeEmoji(text):
    remove_emoji = lambda x: emoji.demojize(str(x)).replace(":", "").strip()
    return remove_emoji(text)

# Aplicar as funções de pré-processamento aos dados

# Remoção de caracteres especiais
dados_filtrados['texto'] = dados_filtrados['texto'].apply(removeCaracteres)

# Remoção de acentos
dados_filtrados['texto'] = dados_filtrados['texto'].apply(removeAcentos)

# Remoção de emojis
dados_filtrados['texto'] = dados_filtrados['texto'].apply(removeEmoji)

# Salvar o DataFrame pré-processado em um novo arquivo CSV no diretório local
dados_filtrados.to_csv('dados_preprocessados.csv', index=False)

print('Dados pré-processados foram salvos com sucesso.')

# Carregar o arquivo CSV pré-processado do diretório local
dados_preprocessados = pd.read_csv('dados_preprocessados.csv')

# Exibir os dados carregados
print(dados_preprocessados)




# # from normalise import Normaliser
# # from nltk.tokenize import word_tokenize
# # from nltk.corpus import stopwords

# # Definição do dicionário de gírias e abreviações
# dicionario_girias = {
#     'vc': 'voce',
#     'vcs':'voce',
#     'Vc': 'voce',
#     'pq': 'porque',
#     'tbm': 'tambem',
#     'q': 'que',
#     'td': 'tudo',
#     'blz': 'beleza',
#     'flw': 'falou',
#     'kd': 'cade',
#     'to': 'estou',
#     'mt': 'muito',
#     'cmg': 'comigo',
#     'ctz': 'certeza',
#     'jah': 'ja',
#     'naum': 'nao',
#     'ta': 'esta',
#     'eh': 'e',
#     'vlw': 'valeu',
#     'p': 'para',
#     'qnd': 'quando',
#     'msm': 'mesmo',
#     'fzr': 'fazer',
#     'agr': 'agora'
# }

# # Função para normalizar o texto substituindo gírias e abreviações
# def normalizar_texto(texto):
#     norm = Normaliser(tokenizer='readable')
#     return norm.normalise(texto)

# # Aplicar a função de normalização do texto
# dados_filtrados['texto_normalizado'] = dados_filtrados['texto'].apply(normalizar_texto)

# # Função para remover stopwords
# def remover_stopwords(texto):
#     tokens = word_tokenize(texto.lower())
#     stop_words = set(stopwords.words('portuguese'))
#     tokens_sem_stopwords = [token for token in tokens if token not in stop_words]
#     return ' '.join(tokens_sem_stopwords)

# # Aplicar a função de remoção de stopwords
# dados_filtrados['texto_sem_stopwords'] = dados_filtrados['texto_normalizado'].apply(remover_stopwords)

# # Salvar o DataFrame com as alterações em um novo arquivo CSV no diretório local
# dados_filtrados.to_csv('dados_preprocessados_continuacao.csv', index=False)

# print('Dados pré-processados da continuação foram salvos com sucesso.')

# # Carregar o arquivo CSV pré-processado da continuação do diretório local
# dados_preprocessados_continuacao = pd.read_csv('dados_preprocessados_continuacao.csv')

# # Exibir os dados carregados
# dados_preprocessados_continuacao





# from nltk.corpus import stopwords
# import pandas as pd

# # Função para remover emojis
# def removeEmoji(texto):
#     # Implementação da remoção de emojis
#     return texto

# # Função para remover caracteres especiais
# def removeCaracteres(texto):
#     # Implementação da remoção de caracteres especiais
#     return texto

# # Função para remover acentos
# def removeAcentos(texto):
#     # Implementação da remoção de acentos
#     return texto

# # Função para normalizar texto (tratamento de gírias e abreviações)
# def normalizar_texto(texto):
#     # Implementação da normalização de texto
#     return texto

# # Função para processar o texto (remoção de emojis, caracteres especiais, acentos, gírias/abreviações e tokenização)
# def processarTexto(texto):
#     # Implementação do processamento do texto
#     return texto

# # Carregar os dados
# dados_filtrados = pd.read_csv('C:\\Users\\Inteli\\Documents\\GitHub\\Projeto5\\src\\Codigo_Fonte\\Backend\\arquivo.csv')

# # Parte do df que será utilizada para o pré-processamento
# dados_filtrados_ = dados_filtrados['texto']

# # Tratamento dos Emojis
# textoEmoji_ = dados_filtrados_.apply(removeEmoji)

# # Removendo os caracteres especiais
# removeCaracter_ = textoEmoji_.apply(removeCaracteres)

# # Removendo acentos
# removeAcento_ = removeCaracter_.apply(removeAcentos)

# # Tratamento de gírias e abreviações
# normTexto = removeAcento_.apply(normalizar_texto)

# # Tokenização
# proceTexto = normTexto.apply(processarTexto)

# # Definir a lista de stop words personalizada
# stop_words = stopwords.words('portuguese')  # Usando a lista de stop words em português do NLTK

# # Aplicar a função de remoção de stop words
# proceTexto = proceTexto.apply(lambda tokens: [token for token in tokens if token not in stop_words])

# # Salvar os arquivos CSV localmente
# proceTexto.to_csv(r'C:\Users\Inteli\Documents\GitHub\Projeto5\src\Codigo_Fonte\Backend\arquivo_processado.csv', index=False)
# dados_filtrados_.to_csv(r'C:\Users\Inteli\Documents\GitHub\Projeto5\src\Codigo_Fonte\Backend\arquivo_tratado.csv', index=False)

# print('Arquivos CSV salvos localmente com sucesso.')


# """## 7.1 Ilustração Pipeline

# Apresenta-se a sequência de etapas que são aplicadas em uma ordem específica para processar dados de forma ilustrativa. Exibindo suas etapas em fluxo, além de exemplos de uso de forma gráfica, na figura abaixo.







































# """# Atividade 8 : Análise Descritiva

# Apresenta-se a etapa inicial e fundamental na análise de dados, fornecendo códigos das características e padrões presentes nos dados coletados. Permitindo uma compreensão inicial dos dados e auxilio na tomada de decisões e na formulação de estratégias com base nas informações disponíveis. Por meio desta, apresenta-se as seguintes informações coletadas.

# ## 8.1 Quantidade de Linhas utilizadas da tabela
# """

# # Antes do tratamento
# quantidadeLinhas = base.shape[0]
# print("A tabela possui", quantidadeLinhas, "linhas.")

# # Depois do tratamento
# quantidadeLinhas = base_tratada.shape[0]
# print("A tabela possui", quantidadeLinhas, "linhas.")

# """## 8.2 Quantidade de palavras na coluna texto"""

# # Antes do tratamento
# totalPalavras = base['texto'].str.split().str.len().sum()

# print("A coluna 'texto' possui", totalPalavras, "palavras no total.")

# # Depois do tratamento
# totalPalavras = base_tratada['texto'].str.split().str.len().sum()

# print("A coluna 'texto' possui", totalPalavras, "palavras no total.")

# """## 8.3 A distribuição de sentimentos positivos, negativos e neutros expressos pelos usuários;"""

# # Antes do tratamento
# contagemSentimento = base['sentimento'].value_counts()

# print("Contagem de sentimentos:")
# print(contagemSentimento)

# # Depois do tratamento
# contagemSentimento = base_tratada['sentimento'].value_counts()

# print("Contagem de sentimentos:")
# print(contagemSentimento)

# """### 8.3.1 gráfico - A distribuição de sentimentos positivos, negativos e neutros expressos pelos usuários;"""

# # Antes do tratamento

# contagemSentimento = base['sentimento'].value_counts()

# plt.pie(contagemSentimento.values, labels=contagemSentimento.index, autopct='%1.1f%%')
# plt.title('Contagem de Sentimentos')
# plt.tight_layout()
# plt.show()

# # depois do tratamento

# contagemSentimento = base_tratada['sentimento'].value_counts()

# plt.pie(contagemSentimento.values, labels=contagemSentimento.index, autopct='%1.1f%%')
# plt.title('Contagem de Sentimentos')
# plt.tight_layout()
# plt.show()

# """## 8.4 Quantidade de autores na base de dados"""

# # Antes do tratamento

# quantidadeAutores = base['autor'].nunique()

# print("A base de dados possui", quantidadeAutores, "autores únicos.")

# # Depois do Tratamento

# quantidadeAutores = base_tratada['autor'].nunique()

# print("A base de dados possui", quantidadeAutores, "autores únicos.")

# """## 8.5 Os usuários que mais realizaram comentários"""

# # Depois do tratamento

# contagem_usuarios = base_tratada['autor'].value_counts()
# top_usuarios = contagem_usuarios.head(5)


# plt.bar(top_usuarios.index, top_usuarios.values)
# plt.xlabel('Usuários')
# plt.ylabel('Quantidade de Comentários')
# plt.title('Top 5 Usuários com Mais Comentários')
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()

# """## 8.6 Quantidade de comentários por Tipo de Interação;"""

# # Antes do tratamento

# contagemSentimento = base['tipoInteracao'].value_counts()

# print("A quantidade de comentários por Tipo de Interação: ")
# print(contagemSentimento)

# # Depois do tratamento

# contagemSentimento = base_tratada['tipoInteracao'].value_counts()

# print("A quantidade de comentários por Tipo de Interação: ")
# print(contagemSentimento)

# """### 8.6.1 Gráfico - Quantidade de comentários por Tipo de Interação;"""


# # Depois do tratamento

# contagem_interacao = base_tratada['tipoInteracao'].value_counts()

# plt.pie(contagem_interacao.values, labels=contagem_interacao.index, autopct='%1.1f%%')
# plt.title('Distribuição dos Tipos de Interação')
# plt.axis('equal')
# plt.show()
